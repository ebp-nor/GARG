{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b825ca5-56f8-463a-b272-e6e31de08c6a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To access material for this workbook please execute the two notebook cells immediately below (e.g. use the shortcut <b>&lt;shift&gt;+&lt;return&gt;</b>). The first cell can be skipped if you are running this notebook locally and have already installed all the necessary packages. The second cell should print out \"Your notebook is ready to go!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc9395-655a-42a0-ac17-ab2b73d009be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pyodide_kernel' in str(get_ipython()):  # specify packages to install under JupyterLite\n",
    "    raise RuntimeError(\"This workbook is not designed to run in JupyterLite. Please use a Colab or local install\")\n",
    "elif 'google.colab' in str(get_ipython()):  # specify package location for loading in Colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %run /content/drive/MyDrive/GARG_workshop/Notebooks/add_module_path.py\n",
    "else:  # install packages on your local machine (-q = \"quiet\": don't print out installation steps)\n",
    "    !python -m pip install -q -r https://github.com/ebp-nor/GARG/raw/main/jlite/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d2d58c-61d9-4286-946d-c717857f2a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load questions etc for this workbook\n",
    "from IPython.display import SVG\n",
    "import tskit\n",
    "import ARG_workshop\n",
    "workbook = ARG_workshop.Workbook2C()\n",
    "display(workbook.setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c65c0e5-86d2-49d8-a8fd-2cce003fd0cf",
   "metadata": {},
   "source": [
    "### Using this workbook\n",
    "\n",
    "This workbook is intended to be used by executing each cell as you go along. Code cells (like those above) can be modified and re-executed to perform different behaviour or additional analysis. You can use this to complete various programming exercises, some of which have associated questions to test your understanding. Exercises are marked like this:\n",
    "<dl class=\"exercise\"><dt>Exercise XXX</dt>\n",
    "<dd>Here is an exercise: normally there will be a code cell below this box for you to work in</dd>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e3c291-8534-408a-9e33-22fb34cc5470",
   "metadata": {},
   "source": [
    "# Workbook 2-C: ARG inference\n",
    "\n",
    "Although ARG inference is a difficult problem, a number of software programs have been developed in recent years that make a reasonable job at ARG inference. Currently, we recommend either [_SINGER_](https://github.com/popgenmethods/SINGER) (for smaller datasets) or [_tsinfer_](https://github.com/tskit-dev/tsinfer) (coupled with [_tsdate_](https://github.com/tskit-dev/tsdate)) for larger datasets. Both of these approaches output tree sequences. You might also like to consider [Relate](https://myersgroup.github.io/relate/), which has been used extensively in analysis of recent human history (and which can convert output, albeit inefficiently, to _tskit_ format). These three software programs have the same basic requirements: phased data and knowledge of the ancestral state, but are based on very different approaches.\n",
    "\n",
    "* ___SINGER___ takes a statistical sample of likely ARGs using MCMC sampling. That means it is pretty computationally intensive, so will only work on sample sizes of tens or hundreds of genomes. As _SINGER_ takes a sampling appraoch, it outputs multiple possible ARGs, which are sampled using the coalescent with recombination as a probabilistic model. Testing suggests that this provides the most accurate results.\n",
    "* ___Tsinfer___, in contrast, creates a single \"best guess\" ARG using a heuristic approach based on ancestor reconstruction. It does not attempt to estimate times of nodes in the ARG, and unlike _SINGER_, it is therefore not dependent on coalescent assumptions. For many purposes, node times are important, and a separate algorithm, _tsdate_, can be used to infer node times under the molecular clock. We will be use extremly recent versions of the two pieces of software, such that their offical documentation may not yet be up-to-date. The major drawback of the _tsinfer_+_tsdate_ approach is that rather than outputting multiple ARGs, it expresses uncertainty by using *polytomies* (nodes with more than one child). Many analysis approaches are not designed for use on trees with polytomies.\n",
    "* ___Relate___ also uses a heuristic approach, which essentially involves building separate trees along the genome and then combining them together (but this results in very inefficient tree sequences, with low amounts of correlation between adjacent trees). It also outputs a single ARG, but polytomies are fully resolved, even in the absence of strong evidence for how to resolve them. The accuracy of results is on a par with the latest _tsinfer_+_tsdate_.\n",
    "\n",
    "In this workbook, we will focus on using the fastest and most efficient method: _tsinfer_ + _tsdate_. Further, advanced workbooks will introduce _SINGER_. To illustrate, we will use the same simulation as we previously encountered, and inpect the quality of the inference by comparing the dates of the nodes below each mutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c043bbba-99ed-4033-b80f-7c864796a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tskit\n",
    "simulated_ts = tskit.load(\"data/chimp_selection.trees\")\n",
    "print(f\"Loaded data for {simulated_ts.num_samples} genomes over {simulated_ts.sequence_length} bp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2dfa32-b75c-4e8d-b9a4-1c12b02b6ffc",
   "metadata": {},
   "source": [
    "For efficiency reasons, _tsinfer_ reads data in the [VCF Zarr](https://github.com/sgkit-dev/vcf-zarr-spec/) format, which is designed to allow rapid processing of large amounts of genetic variant data. It should soon be possible to [convert tree sequences directly to this format](https://github.com/sgkit-dev/bio2zarr/issues/232), but for the moment, this needs to be done the slow way, by outputting to VCF and converting to a `.vcz` file using [vcf2zarr](https://sgkit-dev.github.io/bio2zarr/vcf2zarr/overview.html#sec-vcf2zarr) on the command-line. For ease of use in the workbook, the `ARG_workshop.ts2vcz` function will do the hard work for you. Remember that the data need to be *phased*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd684cb5-824f-47fd-822b-a6e054cf0f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the .vcz file\n",
    "ARG_workshop.ts2vcz(simulated_ts, \"PanTro-chr3-sim.vcz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eb5808-c341-4c66-a580-3ca62f844e30",
   "metadata": {},
   "source": [
    "Details for how to use for the new (currently alpha) version of `tsinfer` are at [https://tskit.dev/tsinfer/docs/latest/usage.html](https://tskit.dev/tsinfer/docs/latest/usage.html). Basically, we need to wrap the `.vcz` file in a `tsinfer.VariantData` object. This allows us to specify the ancestral allele, and mask out problematic sites or samples. In this simulated data we don't need any masking. That is likely to change with real data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eac2b5e-3206-4799-b34a-b7edc0c3c41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsinfer\n",
    "import numpy as np\n",
    "\n",
    "# We must obtain the ancestral allele from somewhere. Here we use that given in the simulation\n",
    "# See https://github.com/tskit-dev/tsinfer/discussions/523 for discussion for real data\n",
    "ancestral_allele=np.array([s.ancestral_state for s in simulated_ts.sites()])\n",
    "\n",
    "# The VariantData interface is now the preferred way to create tsinfer input files (the old one was called SampleData)\n",
    "vdata = tsinfer.VariantData(\"PanTro-chr3-sim.vcz\", ancestral_allele=ancestral_allele)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa203717-7e97-4086-9d6f-ac4af9214c54",
   "metadata": {},
   "source": [
    "### The 3 steps of tsinfer\n",
    "\n",
    "The principle behind _tsinfer_ should have been introduced in a talk. The inference algorithm is split into 3 steps:\n",
    "1. Generating ancestors (ga): [`tsinfer.generate_ancestors()`](https://tskit.dev/tsinfer/docs/latest/api.html#tsinfer.generate_ancestors)\n",
    "2. Matching ancestors (ma): [`tsinfer.match_ancestors()`](https://tskit.dev/tsinfer/docs/latest/api.html#tsinfer.match_ancestors) - this is usually the slowest step, and hardest to parallelize.\n",
    "3. Matching samples (ms): [`tsinfer.match_samples()`](https://tskit.dev/tsinfer/docs/latest/api.html#tsinfer.match_samples)\n",
    "\n",
    "We can run all three in one go by calling `tsinfer.infer()`, but it's more flexible to run them separately, and allows you to save intermediate results, which is useful for large inferences. You can use the `progress_monitor` argument to see how long the steps will take, and the `num_threads` argument to use more CPU cores on your computer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15017ad0-1120-449a-9c0e-97da460b9dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_params = {\n",
    "    \"num_threads\": 6,  # Set to the number of cores on your computer.\n",
    "    \"progress_monitor\": True,\n",
    "}\n",
    "# On my macbook air with num_threads=6, this takes about 2 and a half minutes for this dataset\n",
    "# (34.3 thousand constructed ancestors)\n",
    "ancestors = tsinfer.generate_ancestors(vdata, **general_params)\n",
    "ancestors_ts = tsinfer.match_ancestors(vdata, ancestors, **general_params)\n",
    "ts = tsinfer.match_samples(vdata, ancestors_ts, **general_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a905b076-0479-4a50-a237-44f28edc2521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute code block with <shift>+Return to display question; type and press return, or click on the buttons to answer\n",
    "workbook.question(\"num_trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8de030-5fc6-4bd9-8b94-8c63100330c7",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "The inferred tree sequence should encode exactly the same genotype data as the original (apart from imputation, see below). However, the genealogy is very likely to be different (after all, we are vanishingly unlikely to be able to infer the true ARG). There are four main points to note:\n",
    "\n",
    "* The inferred tree sequence can contain polytomies\n",
    "* The inferred tree sequence will have the same number of sites as the (masked) input, but can have multiple mutations at a site (this can be adjusted using a \"mismatch\" parameter, described later)\n",
    "* The inferred tree sequence has no genealogical information before the first site and after the last site: there are empty trees in those regions (these empty regions can be removed using `ts.trim()`, although that will shift the site positions leftwards.\n",
    "* Missing data in the original dataset is automatically *imputed* in the inferred tree sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea7736-7b6d-4911-b5f1-008b479df210",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"simulation diversity = {simulated_ts.diversity():.6f}, diversity in inferred ts = {ts.diversity():.6f}\")\n",
    "print(f\"simulation num_sites = {simulated_ts.num_sites}, num_sites in inferred ts = {ts.num_sites}\")\n",
    "print(f\"simulation num_mutations = {simulated_ts.num_mutations}, num_mutations in inferred ts = {ts.num_mutations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c7f10d-c2a0-4e93-98e5-a3d70e8fc4d5",
   "metadata": {},
   "source": [
    "<dl class=\"exercise\"><dt>Exercise 1</dt>\n",
    "<dd>Try obtaining a statistic (e.g <code>diversity</code>) on the inferred <code>ts</code>. Then try it again using the *branch length* diversity.</dd>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b791b57-7003-465e-a0ba-11dd50f998c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: try both site and branch mode statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda2e50e-2bbd-404c-988c-d3c255dfdc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute code block with <shift>+Return to display question; type and press return, or click on the buttons to answer\n",
    "workbook.question(\"branchVsite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b0248f-8c21-4fed-928d-a236a1290515",
   "metadata": {},
   "source": [
    "However, we can run topological analysis. Here we repeat the GNN and topology-counting we did with the true genealogy, but using the inferred tree sequence.\n",
    "\n",
    "### GNN\n",
    "\n",
    "The true genealogy had a boring GNN plot (each population completely separate), and we might hope the signal was so strong that we would see exactly the same in the inferred topologies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c080c64e-3b7d-45fb-937e-0894a5f183d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sample_sets = {pop.metadata[\"name\"]: ts.samples(population=pop.id) for pop in ts.populations()}\n",
    "sample_sets = {k: sample_sets[k] for k in [\"bonobo\", \"central\", \"western\"]}  # make sure bonobo first, central next, western last\n",
    "gnn = ts.genealogical_nearest_neighbours(ts.samples(), sample_sets=list(sample_sets.values()))\n",
    "df = pd.DataFrame(gnn, columns=list(sample_sets.keys()))\n",
    "df[\"focal_population\"] = [ts.population(ts.node(u).population).metadata[\"name\"] for u in ts.samples()]\n",
    "mean_gnn = df.groupby(\"focal_population\").mean()\n",
    "sns.clustermap(mean_gnn, col_cluster=False, z_score=0, cmap=\"mako\", cbar_pos=(1.0, 0.05, 0.05, 0.7));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f5d5f6-e4f3-4deb-ab05-e41d76a9006e",
   "metadata": {},
   "source": [
    "### Topology counting\n",
    "\n",
    "The results from counting embedded topologies were a little more subtle. Here we'll also repeat the topology counting code for the simulated data, so it's easier to compare with the inferred tree sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b399e160-d338-4e31-b028-d738ba904c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "true_topology_totals = {tree.rank(): {\"counts\": 0, \"spans\": 0} for tree in tskit.all_trees(3)}\n",
    "inferred_topology_totals = {tree.rank(): {\"counts\": 0, \"spans\": 0} for tree in tskit.all_trees(3)}\n",
    "for topology_totals, tree_seq in [(true_topology_totals, simulated_ts), (inferred_topology_totals, ts)]:\n",
    "    tree_seq = tree_seq.trim()  # remove empty regions before and after\n",
    "    sample_sets = {pop.metadata[\"name\"]: tree_seq.samples(pop.id) for pop in tree_seq.populations()}\n",
    "    for topology_counter, tree in tqdm(zip(\n",
    "        tree_seq.count_topologies(sample_sets.values()),\n",
    "        tree_seq.trees()\n",
    "    ), total=tree_seq.num_trees):\n",
    "        embedded_topologies = topology_counter[0, 1, 2]\n",
    "        weight = tree.span / embedded_topologies.total()\n",
    "        for rank, count in embedded_topologies.items():\n",
    "            topology_totals[rank][\"counts\"] += count\n",
    "            topology_totals[rank][\"spans\"] += count * weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f9e15-ce29-4d4d-bb1c-580be16eaa4e",
   "metadata": {},
   "source": [
    "We'll plot it out using some HTML to make it look nice. If you aren't familiar with raw HTML, don't worry about understanding the cell below: it's simply for formatting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf38742-ab32-46b3-ae3c-66ac17878324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "td = '<td style=\"text-align: center\">'\n",
    "for name, tree_seq, topology_totals in [\n",
    "    (\"True\", simulated_ts, true_topology_totals),\n",
    "    (\"Inferred\", ts, inferred_topology_totals),\n",
    "]:\n",
    "    names = {pop.id: pop.metadata[\"name\"] for pop in tree_seq.populations() if len(tree_seq.samples(pop.id)) > 0}\n",
    "    L = tree_seq.sequence_length\n",
    "    display(HTML(\n",
    "        f\"<h3>{name} data</h3>\" +\n",
    "        \"<table><tr><td>\" +\n",
    "        \"</td><td>\".join([\n",
    "            tskit.Tree.unrank(num_leaves=3, rank=rank).draw_svg(node_labels=names)\n",
    "            for rank in topology_totals.keys()\n",
    "        ]) +\n",
    "        \"</td></tr>\" + \n",
    "        ('<tr>' + td) +\n",
    "        ('</td>' + td).join([\n",
    "            f\"counts: {v['counts']}<br>spans: {v['spans']:.2f}<br>span %: {v['spans']/L * 100:.2f}\"\n",
    "            for v in topology_totals.values()\n",
    "        ]) + \n",
    "        \"</td></tr></table>\"\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0efa5f-eb28-49b6-abae-0ee4ec972523",
   "metadata": {},
   "source": [
    "The simulated data is slightly more noisy than the true data, but it still very strongly supports grouping Western and Central chimps together (98% vs 99% of the genome). Although we don't have a measure for how much difference is \"significant\", it imples that the inference is not doing a bad job as clustering as we expect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a0be6-f2f8-49c7-9440-a35d2563faf2",
   "metadata": {},
   "source": [
    "### Other topological measures\n",
    "\n",
    "Another approach to comparing original and inferred tree sequences is to use tree distance metrics to compare each local tree. The [ts.coiterate(...)](https://tskit.dev/tskit/docs/stable/python-api.html#tskit.TreeSequence.coiterate) method, which jointly moves left-to-right along two comparable tree sequences can help here. However, it is unclear what the best tree metrics to use are. For this reason, we will instead carry out further analysis using dates of nodes and mutations, for which we will need to date the inferred tree sequence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e2a7a5-e8cf-4a27-8933-9906c6361799",
   "metadata": {},
   "source": [
    "## <em>Tsdate</em>: an HMM on a graph\n",
    "\n",
    "You should have been introduced to the concept behind <em>tsdate</em> in a talk. As the [docs](https://tskit.dev/tsdate/docs/latest/methods.html#the-variational-gamma-method) say:\n",
    "<blockquote>The directed graph that represents the genealogy can (in its undirected form) contain cycles, so a technique called “expectation propagation” (EP) is used, in which local estimates to each gamma distribution are iteratively refined until they converge to a stable solution. This comes under a class of approaches sometimes known as “loopy belief propagation”.</blockquote>\n",
    "Running <em>tsdate</em> requires a mutation rate to be specified. Handily, the mutation rate used in the simulation is stored in the <a href=\"https://tskit.dev/tskit/docs/stable/provenance.html\">provenance</a> table of the simulated tree sequence. Running the actual algorithm is fast (although importing tsdate for the first time can take a minute or two). There's no need to parallelise across threads.\n",
    "\n",
    "Note that the output of _tsinfer_ is not fully simplified (it has some nodes with non-coalescent segments). We have found that these can cause problems with dating, so we `tsdate.preprocess_ts` the inferred tree sequence before dating, which simplifies it: see the <a href=\"https://tskit.dev/tsdate/docs/latest/python-api.html#preprocessing-tree-sequences\">docs</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c6896-2fc8-46b4-9bbc-8675f84f8c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsdate\n",
    "import json\n",
    "last_provenance = json.loads(simulated_ts.provenance(-1).record)\n",
    "assert last_provenance[\"parameters\"][\"command\"] == \"sim_mutations\"\n",
    "mutation_rate = last_provenance[\"parameters\"][\"rate\"]\n",
    "undated_ts = tsdate.preprocess_ts(ts)\n",
    "dated_ts = tsdate.date(undated_ts, mutation_rate=mutation_rate, progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ae484-c12f-46e0-93d6-94d649a5b8f2",
   "metadata": {},
   "source": [
    "We can now plot the dates of nodes, or mutations. Of course, the inferred tree sequence will not have eactly the same nodes as the simulated one, so to find equivalent nodes, we simply compare nodes under the mutation at equivalent sites (if there are multiple mutations ata single site, we just take the oldest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5276e751-dc3b-4780-a668-f42f903f171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def node_time_below_oldest_muts(input_ts):\n",
    "    # As there are slightly different numbers of mutations, we can't simply compare mutation times\n",
    "    # Instead, we compare the oldest mutation at each site, and take the time of the node below that\n",
    "    return np.array([\n",
    "        input_ts.nodes_time[s.mutations[0].node] if len(s.mutations) else np.nan\n",
    "        for s in input_ts.sites()\n",
    "    ])\n",
    "\n",
    "def plot_log_times(orig_ts, new_ts):\n",
    "    orig_time_oldest_mut_node = node_time_below_oldest_muts(orig_ts)\n",
    "    new_time_oldest_mut_node = node_time_below_oldest_muts(new_ts)\n",
    "    use = np.logical_and(orig_time_oldest_mut_node > 0, new_time_oldest_mut_node > 0)\n",
    "    x = np.log10(orig_time_oldest_mut_node[use])\n",
    "    y = np.log10(new_time_oldest_mut_node[use])\n",
    "    plt.hexbin(x, y, bins='log')\n",
    "    plt.axline((0, 0), slope=1, color=\"red\")\n",
    "    plt.xlabel(\"True node time (generations)\")\n",
    "    plt.ylabel(\"Inferred node time (generations)\")\n",
    "    plt.text(0, 5, f\"r2 = {np.corrcoef(x, y)[0,1] ** 2:.5f}\")\n",
    "    # set log ticks\n",
    "    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x,y: f'{10**x:.0f}'))\n",
    "    plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x,y: f'{10**x:.0f}'))\n",
    "\n",
    "plot_log_times(simulated_ts, dated_ts)\n",
    "plt.title(\"First round of inference\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f558888-7b35-4320-9f91-f8374bbbe3e7",
   "metadata": {},
   "source": [
    "It appears as if there is banding in the true times, presumably caused by the bottlenecking, as we saw in the true (simulated) tree sequence. Let's see how well we can spot the demographic and selection patterns in plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61cfe75-4a44-43e8-b698-9ef602873468",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARG_workshop.edge_plot(dated_ts, width=15, plot_hist=True, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c347a-2dd0-4d89-918d-eb15c233c633",
   "metadata": {},
   "source": [
    "And separated by population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5177edad-dd96-4f9f-b476-36829f6a1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, gridspec_kw={\"width_ratios\": [8, 1], \"hspace\": 0.3}, figsize=(15, 10), sharey=True)\n",
    "for ax_row, pop in zip(axes, dated_ts.populations()):\n",
    "    xaxis = (pop.id==dated_ts.population(-1).id)\n",
    "    ARG_workshop.edge_plot(dated_ts.simplify(dated_ts.samples(population=pop.id)), ax=ax_row, xaxis=xaxis, title=pop.metadata[\"name\"], alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f75115-43a1-41c8-a515-773d44302a6b",
   "metadata": {},
   "source": [
    "How about pairwise coalescence plots? First, averaged over the whole geneme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e4661e-05db-4848-9f78-8262136f5a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_coalescence_rates(input_ts, sample_sets=None, time_breaks=None, window_breaks=None):\n",
    "    # NB: in the next tskit release (0.5.9), there will be an API change such that\n",
    "    # this function will be directly available as `ts.pair_coalescence_rates(time_breaks)`\n",
    "    if sample_sets is not None:\n",
    "        sample_sets = [list(s) for s in sample_sets]  # work around small bug in implementation of coalescence_time_distribution\n",
    "    d = input_ts.coalescence_time_distribution(\n",
    "        sample_sets=sample_sets,\n",
    "        window_breaks=window_breaks,\n",
    "        weight_func=\"pair_coalescence_events\",\n",
    "    )\n",
    "    return d.coalescence_rate_in_intervals(np.array(time_breaks))\n",
    "\n",
    "time_windows = np.logspace(0, np.log10(dated_ts.max_time), 30)\n",
    "rates = pair_coalescence_rates(dated_ts, time_breaks=time_windows)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "# This might complain if any rate is 0: that can be ignored\n",
    "for ax, ylabel, y in zip(axes, (\"Instantaneous Coalescence Rate (ICR)\", \"Inverse ICR (IICR)\"), (rates, 1/rates)):\n",
    "    ax.stairs(y.flatten(), time_windows, baseline=None)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(\"Time ago {dated_ts.time_units}\")\n",
    "    ax.set_ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fbc6fc-ead1-4fa4-84d3-885a2f013bd5",
   "metadata": {},
   "source": [
    "And now with a \"local\" (along the genome) plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a51a8-f168-43b4-a837-8fbbbfdbe8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pair_rates(input_ts, genomic_windows, num_log_timebins, sample_sets=None, indexes=None, axes=None):\n",
    "    # indexes is a list of tuple pairs, e.g. [(0, 1), (1, 2)]\n",
    "    time_breaks = np.logspace(0, np.log10(input_ts.max_time), num_log_timebins)\n",
    "    rates = pair_coalescence_rates(input_ts, sample_sets, window_breaks=genomic_windows, time_breaks=time_breaks)\n",
    "    if sample_sets is None:\n",
    "        sample_sets = [input_ts.samples()]\n",
    "    order = [(a, b) for a in range(len(sample_sets)) for b in range(a, len(sample_sets))]\n",
    "    if indexes is None:\n",
    "        indexes = np.arange(len(order))\n",
    "    else:\n",
    "        indexes = [order.index(i) for i in indexes]\n",
    "    if axes is None:\n",
    "        fig, axes = plt.subplots(len(indexes), 1, figsize=(12.5, 3 * len(indexes)))\n",
    "    num_axes = 1\n",
    "    try:\n",
    "        num_axes = len(axes)\n",
    "    except TypeError:\n",
    "        axes = [axes]\n",
    "    if num_axes != len(indexes):\n",
    "        raise ValueError(\"Must have same number of axes as indexes\")\n",
    "    for ax, rate in zip(axes, (rates[i] for i in indexes)):\n",
    "        im = ax.pcolormesh(genomic_windows, time_breaks, rate)\n",
    "        ax.set_yscale(\"log\")\n",
    "        bar = plt.colorbar(im, ax=ax)\n",
    "        bar.ax.set_ylabel('pairwise coalescent density', labelpad=10, rotation=270)\n",
    "        ax.set_ylabel(f\"Time ({input_ts.time_units})\");\n",
    "\n",
    "genomic_windows = np.linspace(0, dated_ts.sequence_length, 30)\n",
    "plot_pair_rates(dated_ts.simplify(), genomic_windows, num_log_timebins=20)\n",
    "plt.xlabel(\"Genome position\")\n",
    "plt.ylabel(f\"Time ({dated_ts.time_units})\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de63ca-7fe4-46b5-b6ed-0b00e7ecf30d",
   "metadata": {},
   "source": [
    "<dl class=\"exercise\"><dt>Exercise 2</dt>\n",
    "<dd>Plot the cross-coalescence plots between the two populations, using the code in workbook 2A. Remember to use <code>dated_ts</code> not <code>ts</code></dd>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3966d974-9b75-4545-af5a-2feeabca0bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: have a look at the cross coalescence plots ()\n",
    "ts = None  # remove references to the old \"ts\" variable: you should be using dated_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6385645-9cdf-4cd9-87d7-fd494e813309",
   "metadata": {},
   "source": [
    "## Extension\n",
    "\n",
    "In our experience, sometimes we can improve the _tsinfer_ step of inference by using the dates inferred from _tsdate_, rather than using frequency as a proxy for local time order. To do this, we can use the `sites_time` parameter when wrapping the `.vcz` file in a `VariantData` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32009e7f-773c-40a6-abbd-030e0129ae2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If we want to change anything (e.g. use different times), we can simply make a new VariantData object (this is efficient and instantaneous)\n",
    "vdata = tsinfer.VariantData(\"PanTro-chr3-sim.vcz\", ancestral_allele=ancestral_allele, sites_time=node_time_below_oldest_muts(dated_ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e655f-71f4-43b3-90e0-f70c90420dd5",
   "metadata": {},
   "source": [
    "Note that this could take a long time (e.g. an hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f476498d-51d4-4c47-85f3-c58c56f5f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll demo the use of the `tsinfer.infer()` command, which rolls all 3 steps of\n",
    "# inference into one.\n",
    "reinferred_ts = tsinfer.infer(vdata, num_threads=4, progress_monitor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7609765-22c4-4803-80b2-33633dfad26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "undated_ts = tsdate.preprocess_ts(reinferred_ts)\n",
    "redated_ts = tsdate.date(undated_ts, mutation_rate=mutation_rate, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af931b7-5689-41dd-951a-a64c93a0a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log_times(simulated_ts, redated_ts)\n",
    "plt.title(\"After one round of redating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6abeea-2f94-4da4-9ead-4275a26e2e97",
   "metadata": {},
   "source": [
    "Plotting this new redated tree sequence is left as an exercise to the user!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
